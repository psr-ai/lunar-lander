\section{Model Description}
\label{sec:Model}

Our model is Deep Reinforcement Learning, and the algorithms are variants of Deep Q-Network (DQN). DQN is a model-free learning agent.

\subsection{Framework}
Basic framework is defined in Fig 1. The agent performs action($a$) on the environment. Performing the action on the environment returns reward($r$) and new state $s'$. Our agent then incorporates this information.

\begin{figure}%
\centering
\includegraphics[scale=0.3]{reinforcement_framework.png}%
\caption{Reinforcement Learning framework}%
\label{fig:datastats}%
\end{figure}

In such a model, for our problem, here is quick description of notations:
\begin{enumerate}
\item $S$ is defined as all possible states, $s$ is one particular 8 dimensional state
\item $A$ is defined as all possible actions, $a$ is one of the actions out of $[\text{idle}, \text{fire engine}, \text{rotate left}, \text{rotate right}]$
\item $R$ is the reward distribution given $(s, a)$
\item $P$ is the set of possible transitions and their probabilities given $(s, a)$
\item $\gamma$: the discount factor. How much we want our agent to discount future reward. It is a hyperparameter that we define ourselves.
\end{enumerate}

\subsection{Goal}

Our model builds off of Q-learning algorithms by using a Deep Neural Network (DNN) for approximating the state-action Q-value, Q(s, a). 
Given the state $s$, our goal is to identify a policy $\pi_{opt}$ that maps the states to actions in order to maximize the total reward we get.

\begin{center}
S $\rightarrow$ A based on $\pi_{opt}$
\end{center}

As we know, $Q$ value is defined as the expected reward that we get following action $a$ in a given state $s$ and then following the policy $\pi$, our objective is to define a $Q_{opt} (s, a)$, which can be maximised over all possible actions at a state $s$, in order to find the $\pi_{opt}$.

\section{Algorithm Description}

\subsection{Q-Learning}
Q-learning learns action-reward function Q(s,a): determines best action to take in a particular state. In Q-learning we build memory table Q[s,a] to store Q-values for all combinations a, as and when we encounter state $s$. We sample an action from the current state to find out reward  and new state. From the memory table, we determine the next action to take which has maximum Q(s,a).

\begin{figure}%
\centering
\includegraphics[width=0.6\columnwidth]{figures/Q-learning.png}%
\caption{Q-Learning Algorithm}%
\label{fig:datastats}%
\end{figure}


\subsection{DQN}
The number of actions we can take from current state is large and we need to observe each action space to solve this problem. We will be using Deep Q Network (DQN) to find Q(s,a). We will be updating model parameters for a mini-batch. To solve this challenge we can slow down the changing Q-value using Experience replay and Target network.
\newline
We will train the neural network on subset of transitions into a buffer. From this buffer we will sample mini batch which will be stable for training. 

\begin{figure}%
\centering
\includegraphics[width=1.0\columnwidth]{figures/DQN-ExperinceReplay.png}%
\caption{DQN with Experience Replay}%
\label{fig:datastats}%
\end{figure}



\subsection{Architecture}

Input to the neural network used in DQN is a 8 dimensional vector followed by two dense layers and then by fully connected layers to compute Q value for each of the 4 actions.
\begin{figure}%
\centering
\includegraphics[width=1.0\columnwidth]{figures/DNN_image.png}%
\caption{Neural Network The Deep Q neural network topology, consisting of 8 input nodes, 128-node hidden layer, a 64-node hidden layer, and 4 output nodes.}%
\label{fig:datastats}%
\end{figure}

\subsection{Experiments and Evaluation}

Based on our initial experiments, we got an average reward of close to 200 consistently after about 2000 episodes. In future, we will try to do more feature engineering and try out variants of DQN network to reduce the learning time.

\begin{figure}%
\centering
\includegraphics[width=0.8\columnwidth]{figures/tensorboard.png}%
\caption{Initial Experimental Results}%
\label{fig:datastats}%
\end{figure}

\begin{figure}%
\centering
\includegraphics[width=0.6\columnwidth]{figures/evaluate_100.png}%
\caption{Rewards per Episode}%
\label{fig:datastats}%
\end{figure}


