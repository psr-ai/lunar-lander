\section{Infrastructure}



We have used the OpenAI gym \citep{openai} library to train our agent. Although some insights are provided in Box2D Lunar Lander on the OpenAI website, thorough exploration of actions, state space, environment etc. was done before starting to solve the problem. Following is the description:

\subsection{Actions}
In this game, four discrete actions are available to the playing agent at any time frame: 

\begin{enumerate}[label=(\alph*)]
\item  Do nothing 
\item (b)Fire left orientation engine (rotates the lunar lander clockwise)
\item (c) Fire main engine (provides upward thrust) 
\item  Fire right orientation engine (rotates the lunar lander anti-clockwise) 
\end{enumerate}

The agent can choose only one action among the given actions at a given time frame.

\subsection{Terrain} 
The terrain is a combination of 10 points, and the helipad(landing zone) is fixed between 5th and 6th points towards the center. The values of the height of the landing zone(5th and 6th points on the terrain) are viewport height divided by 4, and the rest of the points are randomly sampled between 0 to H/2 using \textit{numpy} random and smoothened (averaging 3 continuous points).

\subsection{ State}

The state is 8 dimensional values of different parameters of the lunar lander at any given time. The starting state is randomly initialized (the lunar lander takes a step in the world through the "idle" action) with certain bounds based on the environment. The state space is as follows

\begin{enumerate}
\item Position of LunarLander w.r.t X-axis
\item Position of LunarLander w.r.t Y-axis
\item Velocity along X-axis 
\item Velocity along Y-axis
\item LunarLander Angle
\item Angular velocity 
\item Left leg contacted the surface (Initial value: False)
\item Right leg contacted the surface (Initial value: False)
\end{enumerate}

\subsection{End State}

When the lunar lander stabilises on the landing surface (change in shape of lunar lander is constantly 0 for a number of frames and speed is 0). 


\subsection{Rewards and Transitions}

Before defining the rewards, let's define the shape of the lunar lander which decides the rewards. The shape of the lunar lander is a function of position coordinates $(x, y)$, linear velocities $(v_x, v_y)$, lander angle $\theta$ and contact of both the lander legs. We are interested in finding the change of shape at every step for the lunar lander to calculate the rewards for each given action. Shape change is given by subtraction of previous shape and current shape. Formally, shaping at time frame $t$:

\begin{align*}
\text{shaping}_{t} = &- 100*(x^2 + y^2) \\
           & - 100*(v_x^2 + v_y^2) \\
            &- 100*abs(\theta) + 10*(\text{Left leg contacted}) + 10*(\text{Right leg contacted}) \\
\text{shape change} = & \text{shaping}_t - \text{shaping}_{t-1}
\end{align*}

The rewards are defined as follows:

\begin{enumerate}
\item If the lunar lander crashes, or goes out of the bounds: $-100$
\item If the lunar lander is not awake anymore (stabilises at 0 shape change): $+100$
\item Doing nothing: shape change 
\item Firing the engine: shape change - 0.3
\item Rotating: shape change - 0.03
\end{enumerate}

The total reward will automatically be a sum of all the rewards at each time frame, and if the lander touches the ground with it's legs, will add those rewards to the total rewards earned during an episode. Transition probabilities are unknown, we get next states by simulating the lunar lander in the box environment given the current state and action taken. \\

\textbf{Note}: The transition probabilities and rewards are unknown to our agent, which it will try to figure out through exploration and incorporate in learning. 

