%\documentclass{vgtc}
% commented
%\usepackage{amsmath, url, graphicx, algorithm, ctable, times}
%\usepackage{algpseudocode}
%\usepackage{enumitem}

% newly added per NIPS
%\documentclass{article} % For LaTeX2e
%\usepackage{hyperref}
%\usepackage{url}
%

\documentclass{article}
\usepackage{fullpage,enumitem,amsmath,
amssymb,graphicx,url,listings,color,hyperref,floatrow,natbib,pdfpages}


%\definecolor{mygreen}{RGB}{28,172,0}  % color values Red, Green, Blue
%\definecolor{mylilas}{RGB}{170,55,241}

%\usepackage{nips13submit_e,times}

%\makeatletter
%\renewcommand{\ALG@beginalgorithmic}{\small}
%\makeatother

\usepackage[caption=false]{subfig}
%\marginsize{1.5cm}{1.5cm}{1cm}{1cm}
\captionsetup{font = scriptsize}
%\makesavenoteenv{tabular}


\begin{document}

\pagestyle{empty} 
%\title{Large AI Agent for Lunar Lander}

\title{\textbf{ AI Agent for Lunar Lander}}

\author{Prabhjot Singh Rai  (prabhjot) \\ 
\and Abhishek Bharani (abharani) \\
\and Amey Naik (ameynaik)
}


\maketitle

\input{task_definition}

\input{infrastructure}
\input{approach}
\input{experiments}
\input{conclusion}
\input{codalab}
%\pagestyle{plain}

\bibliography{biblio}
\bibliographystyle{abbrv}
%\newpage
%%\input{contribution}

\section{Appendix}
\subsection{Implementation choices}

The choice of state was based on the actual configuration of the lunar lander, and we learnt weights for each feature in the state. Another choice could have been training on image data, feeding images per episode and learning on gained rewards. Since this would have been computationally expensive, we chose the former approach to explicit definition of state space and learning it's weights. \\

For running different algorithms, we created separate classes so that the code is not only modularized, but can also be run easily on different openai environments, learning algorithms can be easily changed etc. Here's a quick description of different classes:

\begin{enumerate}
\item \textbf{Brain:} Class which contains keras models, updates the weights through train function and performs prediction based on learnt weights. 
\item \textbf{Memory:} Class which appends observations until maximum memory length and samples based on given batch size hyperparameter
\item \textbf{Agent:} Our agent class which explores and exploits based on fixed hyperparameters(gamma, epsilon max, epsilon min and decay) and passed arguments. This is also the class where we are performing the replay action and training the agent's brain instance. It also contains another instance of memory class which is used in replay while sampling.
\item \textbf{Environment:} Class which runs the episode on given agent and asks the agent to observe and replay whenever the agent is trying to learn on episodes. It returns the information on how much reward was observed on each episode and for how long each episode ran
\end{enumerate}

\subsection{Webpage for the project}

https://www.prabhjotrai.com/lunar-lander
\end{document}

